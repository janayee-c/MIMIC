{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "class RadGraphEmbedder:\n",
    "    def __init__(self, clinical_bert_model=\"emilyalsentzer/Bio_ClinicalBERT\"):\n",
    "        \"\"\"\n",
    "        Initialize the RadGraph embedder with a clinical BERT model\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(clinical_bert_model)\n",
    "        self.bert_model = AutoModel.from_pretrained(clinical_bert_model)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.bert_model.to(self.device)\n",
    "        \n",
    "    def process_radgraph(self, nodes, edges):\n",
    "        \"\"\"\n",
    "        Convert RadGraph nodes and edges into a PyG graph\n",
    "        \n",
    "        Args:\n",
    "            nodes: Dict of node_id -> {text, type, span}\n",
    "            edges: List of (source_id, target_id, relation_type)\n",
    "            \n",
    "        Returns:\n",
    "            torch_geometric.data.Data object\n",
    "        \"\"\"\n",
    "        # Generate BERT embeddings for each node\n",
    "        node_embeddings = []\n",
    "        node_types = []\n",
    "        \n",
    "        for node_id in sorted(nodes.keys()):\n",
    "            node = nodes[node_id]\n",
    "            # Get BERT embedding for node text\n",
    "            inputs = self.tokenizer(node['text'], \n",
    "                                  return_tensors='pt',\n",
    "                                  padding=True,\n",
    "                                  truncation=True,\n",
    "                                  max_length=128)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.bert_model(**inputs)\n",
    "                # Use CLS token embedding as node representation\n",
    "                node_emb = outputs.last_hidden_state[:, 0, :].cpu()\n",
    "                node_embeddings.append(node_emb)\n",
    "            \n",
    "            # Convert node type to one-hot\n",
    "            type_mapping = {\n",
    "                'ANATOMY': 0,\n",
    "                'OBSERVATION': 1,\n",
    "                'MEASUREMENT': 2,\n",
    "                'PROCEDURE': 3,\n",
    "                'MODIFIER': 4\n",
    "            }\n",
    "            node_type = torch.zeros(len(type_mapping))\n",
    "            node_type[type_mapping[node['type']]] = 1\n",
    "            node_types.append(node_type)\n",
    "            \n",
    "        node_embeddings = torch.cat(node_embeddings, dim=0)\n",
    "        node_types = torch.stack(node_types)\n",
    "        \n",
    "        # Combine BERT embeddings with node type\n",
    "        node_features = torch.cat([node_embeddings, node_types], dim=1)\n",
    "        \n",
    "        # Create edge index and edge attributes\n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        relation_mapping = {\n",
    "            'SUGGESTIVE_OF': 0,\n",
    "            'LOCATED_AT': 1, \n",
    "            'MODIFY': 2,\n",
    "            'RELATED_TO': 3\n",
    "        }\n",
    "        \n",
    "        for source, target, rel_type in edges:\n",
    "            edge_index.append([source, target])\n",
    "            edge_type = torch.zeros(len(relation_mapping))\n",
    "            edge_type[relation_mapping[rel_type]] = 1\n",
    "            edge_attr.append(edge_type)\n",
    "            \n",
    "        edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "        edge_attr = torch.stack(edge_attr)\n",
    "        \n",
    "        return Data(x=node_features, \n",
    "                   edge_index=edge_index,\n",
    "                   edge_attr=edge_attr)\n",
    "\n",
    "class InterpretableGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3):\n",
    "        \"\"\"\n",
    "        Interpretable GCN for processing RadGraph embeddings\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Input feature dimension\n",
    "            hidden_channels: Hidden layer dimension\n",
    "            out_channels: Output dimension (number of clinical outcomes)\n",
    "            num_layers: Number of GCN layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        self.attention = torch.nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.attention.append(torch.nn.Linear(hidden_channels, 1))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "            self.attention.append(torch.nn.Linear(hidden_channels, 1))\n",
    "            \n",
    "        # Output layer\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels))\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(out_channels))\n",
    "        self.attention.append(torch.nn.Linear(out_channels, 1))\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        \"\"\"\n",
    "        Forward pass with attention weights for interpretability\n",
    "        \"\"\"\n",
    "        attention_weights = []\n",
    "        \n",
    "        for conv, batch_norm, attn in zip(self.convs, self.batch_norms, self.attention):\n",
    "            # Graph convolution\n",
    "            x = conv(x, edge_index)\n",
    "            x = batch_norm(x)\n",
    "            x = torch.relu(x)\n",
    "            \n",
    "            # Calculate attention weights\n",
    "            attn_weight = torch.sigmoid(attn(x))\n",
    "            attention_weights.append(attn_weight)\n",
    "            \n",
    "            # Apply attention\n",
    "            x = x * attn_weight\n",
    "            \n",
    "        # Global pooling\n",
    "        out = global_mean_pool(x, batch)\n",
    "        \n",
    "        return out, attention_weights\n",
    "    \n",
    "    def interpret_predictions(self, graph_data, predictions, attention_weights):\n",
    "        \"\"\"\n",
    "        Generate interpretations for predictions\n",
    "        \n",
    "        Returns dictionary mapping outcomes to most influential nodes/edges\n",
    "        \"\"\"\n",
    "        interpretations = {}\n",
    "        \n",
    "        # Combine attention weights across layers\n",
    "        combined_attention = torch.cat(attention_weights, dim=1).mean(dim=1)\n",
    "        \n",
    "        # Get top-k most important nodes\n",
    "        k = min(5, len(combined_attention))\n",
    "        top_nodes = combined_attention.topk(k)\n",
    "        \n",
    "        for outcome_idx, pred in enumerate(predictions):\n",
    "            important_nodes = []\n",
    "            for node_idx in top_nodes.indices:\n",
    "                node_importance = {\n",
    "                    'node_idx': node_idx.item(),\n",
    "                    'attention': combined_attention[node_idx].item(),\n",
    "                    'node_features': graph_data.x[node_idx].tolist()\n",
    "                }\n",
    "                important_nodes.append(node_importance)\n",
    "                \n",
    "            interpretations[f'outcome_{outcome_idx}'] = {\n",
    "                'prediction': pred.item(),\n",
    "                'important_nodes': important_nodes\n",
    "            }\n",
    "            \n",
    "        return interpretations\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=100):\n",
    "    \"\"\"\n",
    "    Train the GCN model\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    best_val_auc = 0\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out, _ = model(batch.x, batch.edge_index, \n",
    "                          batch.edge_attr, batch.batch)\n",
    "            loss = criterion(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                out, _ = model(batch.x, batch.edge_index,\n",
    "                             batch.edge_attr, batch.batch)\n",
    "                val_preds.append(torch.sigmoid(out))\n",
    "                val_labels.append(batch.y)\n",
    "                \n",
    "        val_preds = torch.cat(val_preds, dim=0)\n",
    "        val_labels = torch.cat(val_labels, dim=0)\n",
    "        val_auc = roc_auc_score(val_labels, val_preds)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {total_loss:.4f}, Val AUC = {val_auc:.4f}\")\n",
    "            \n",
    "    return model\n",
    "\n",
    "def predict_outcomes(model, graph_data):\n",
    "    \"\"\"\n",
    "    Generate predictions and interpretations for a single graph\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions, attention_weights = model(graph_data.x,\n",
    "                                            graph_data.edge_index,\n",
    "                                            graph_data.edge_attr,\n",
    "                                            graph_data.batch)\n",
    "        probabilities = torch.sigmoid(predictions)\n",
    "        \n",
    "        interpretations = model.interpret_predictions(\n",
    "            graph_data, probabilities, attention_weights)\n",
    "        \n",
    "    return probabilities, interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Features:\n",
      "\n",
      "observations:\n",
      "{'1': {'text': 'acute', 'certainty': 'definitely absent', 'locations': [], 'modifiers': ['process'], 'suggestions': []}, '3': {'text': 'process', 'certainty': 'definitely absent', 'locations': ['cardiopulmonary'], 'modifiers': [], 'suggestions': []}, '4': {'text': 'moderate', 'certainty': 'definitely present', 'locations': [], 'modifiers': ['hiatal hernia'], 'suggestions': []}, '5': {'text': 'hiatal hernia', 'certainty': 'definitely present', 'locations': [], 'modifiers': [], 'suggestions': []}}\n",
      "\n",
      "patterns:\n",
      "{'findings': {'process': ['cardiopulmonary']}, 'locations': {}, 'modifiers': {'process': ['acute'], 'hiatal hernia': ['moderate']}}\n",
      "\n",
      "certainty_analysis:\n",
      "{'definitely absent': 2, 'definitely present': 3}\n",
      "\n",
      "graph_metrics:\n",
      "{'num_entities': 5, 'num_relations': 3, 'num_anatomical_sites': 1, 'num_observations': 4}\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from typing import Dict, List, Tuple, Set\n",
    "from collections import defaultdict\n",
    "\n",
    "class RadGraphAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes RadGraph extracts according to the official schema for pneumonia classification\n",
    "    \"\"\"\n",
    "    \n",
    "    ENTITY_TYPES = {\n",
    "        'Anatomy::definitely present',\n",
    "        'Observation::definitely present',\n",
    "        'Observation::uncertain',\n",
    "        'Observation::definitely absent'\n",
    "    }\n",
    "    \n",
    "    RELATION_TYPES = {\n",
    "        'suggestive_of',  # Observation -> Observation\n",
    "        'located_at',     # Observation -> Anatomy\n",
    "        'modify'          # Observation -> Observation or Anatomy -> Anatomy\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pneumonia_terms = {\n",
    "            'consolidation', 'opacity', 'infiltrate', 'pneumonia',\n",
    "            'airspace disease', 'ground glass', 'patchy', 'focal'\n",
    "        }\n",
    "        \n",
    "    def parse_radgraph_format(self, extract: Dict) -> Tuple[List[Dict], List[Dict]]:\n",
    "        \"\"\"\n",
    "        Converts RadGraph extract format into structured entities and relations\n",
    "        \n",
    "        Args:\n",
    "            extract: RadGraph dictionary extract\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (entities, relations)\n",
    "        \"\"\"\n",
    "        text = extract['text']\n",
    "        entities_dict = extract['entities']\n",
    "        \n",
    "        entities = []\n",
    "        relations = []\n",
    "        \n",
    "        # Process entities\n",
    "        for entity_id, entity_info in entities_dict.items():\n",
    "            entity = {\n",
    "                'id': entity_id,\n",
    "                'text': ' '.join(text.split()[entity_info['start_ix']:entity_info['end_ix'] + 1]),\n",
    "                'type': entity_info['label'],\n",
    "                'start_ix': entity_info['start_ix'],\n",
    "                'end_ix': entity_info['end_ix']\n",
    "            }\n",
    "            entities.append(entity)\n",
    "            \n",
    "            # Process relations\n",
    "            for rel_type, target_id in entity_info.get('relations', []):\n",
    "                relation = {\n",
    "                    'source': entity_id,\n",
    "                    'target': target_id,\n",
    "                    'type': rel_type\n",
    "                }\n",
    "                relations.append(relation)\n",
    "        \n",
    "        return entities, relations\n",
    "\n",
    "    def analyze_certainty(self, entities: List[Dict]) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Analyzes certainty levels of observations\n",
    "        \"\"\"\n",
    "        certainty_counts = defaultdict(int)\n",
    "        for entity in entities:\n",
    "            if '::' in entity['type']:\n",
    "                certainty_counts[entity['type'].split('::')[1]] += 1\n",
    "        return dict(certainty_counts)\n",
    "\n",
    "    def find_pneumonia_patterns(self, entities: List[Dict], relations: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Identifies patterns relevant to pneumonia classification\n",
    "        \"\"\"\n",
    "        patterns = {\n",
    "            'findings': defaultdict(list),\n",
    "            'locations': defaultdict(list),\n",
    "            'modifiers': defaultdict(list)\n",
    "        }\n",
    "        \n",
    "        # Create entity lookup\n",
    "        entity_map = {e['id']: e for e in entities}\n",
    "        \n",
    "        for relation in relations:\n",
    "            source = entity_map[relation['source']]\n",
    "            target = entity_map[relation['target']]\n",
    "            \n",
    "            if relation['type'] == 'located_at':\n",
    "                if 'Observation' in source['type']:\n",
    "                    patterns['findings'][source['text']].append(target['text'])\n",
    "                    \n",
    "            elif relation['type'] == 'modify':\n",
    "                if 'Observation' in source['type'] and 'Observation' in target['type']:\n",
    "                    patterns['modifiers'][target['text']].append(source['text'])\n",
    "                    \n",
    "            elif relation['type'] == 'suggestive_of':\n",
    "                if any(term in source['text'].lower() for term in self.pneumonia_terms):\n",
    "                    patterns['findings']['suggestive_patterns'].append(\n",
    "                        (source['text'], target['text'])\n",
    "                    )\n",
    "        \n",
    "        return {k: dict(v) for k, v in patterns.items()}\n",
    "\n",
    "    def extract_features(self, extract: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Extracts features relevant to pneumonia classification\n",
    "        \n",
    "        Args:\n",
    "            extract: RadGraph dictionary extract\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of extracted features\n",
    "        \"\"\"\n",
    "        entities, relations = self.parse_radgraph_format(extract)\n",
    "        \n",
    "        # Create graph representation\n",
    "        G = nx.DiGraph()\n",
    "        for entity in entities:\n",
    "            G.add_node(entity['id'], **entity)\n",
    "        for relation in relations:\n",
    "            G.add_edge(relation['source'], relation['target'], type=relation['type'])\n",
    "            \n",
    "        # Analyze patterns\n",
    "        patterns = self.find_pneumonia_patterns(entities, relations)\n",
    "        certainty = self.analyze_certainty(entities)\n",
    "        \n",
    "        # Extract observations and their properties\n",
    "        observations = defaultdict(lambda: {\n",
    "            'text': '',\n",
    "            'certainty': '',\n",
    "            'locations': [],\n",
    "            'modifiers': [],\n",
    "            'suggestions': []\n",
    "        })\n",
    "        \n",
    "        for entity in entities:\n",
    "            if 'Observation' in entity['type']:\n",
    "                obs_id = entity['id']\n",
    "                observations[obs_id]['text'] = entity['text']\n",
    "                observations[obs_id]['certainty'] = entity['type'].split('::')[1]\n",
    "                \n",
    "                # Find connected entities\n",
    "                for _, neighbor, rel_data in G.edges(obs_id, data=True):\n",
    "                    neighbor_data = G.nodes[neighbor]\n",
    "                    if rel_data['type'] == 'located_at':\n",
    "                        observations[obs_id]['locations'].append(neighbor_data['text'])\n",
    "                    elif rel_data['type'] == 'modify':\n",
    "                        observations[obs_id]['modifiers'].append(neighbor_data['text'])\n",
    "                    elif rel_data['type'] == 'suggestive_of':\n",
    "                        observations[obs_id]['suggestions'].append(neighbor_data['text'])\n",
    "        \n",
    "        features = {\n",
    "            'observations': dict(observations),\n",
    "            'patterns': patterns,\n",
    "            'certainty_analysis': certainty,\n",
    "            'graph_metrics': {\n",
    "                'num_entities': len(entities),\n",
    "                'num_relations': len(relations),\n",
    "                'num_anatomical_sites': len([e for e in entities if 'Anatomy' in e['type']]),\n",
    "                'num_observations': len([e for e in entities if 'Observation' in e['type']])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Example RadGraph extract\n",
    "    sample_extract = {\n",
    "        'text': 'no evidence of acute cardiopulmonary process moderate hiatal hernia',\n",
    "        'entities': {\n",
    "            '1': {'tokens': 'acute', 'label': 'Observation::definitely absent', \n",
    "                  'start_ix': 3, 'end_ix': 3, 'relations': [['modify', '3']]},\n",
    "            '2': {'tokens': 'cardiopulmonary', 'label': 'Anatomy::definitely present', \n",
    "                  'start_ix': 4, 'end_ix': 4, 'relations': []},\n",
    "            '3': {'tokens': 'process', 'label': 'Observation::definitely absent', \n",
    "                  'start_ix': 5, 'end_ix': 5, 'relations': [['located_at', '2']]},\n",
    "            '4': {'tokens': 'moderate', 'label': 'Observation::definitely present', \n",
    "                  'start_ix': 6, 'end_ix': 6, 'relations': [['modify', '5']]},\n",
    "            '5': {'tokens': 'hiatal hernia', 'label': 'Observation::definitely present', \n",
    "                  'start_ix': 7, 'end_ix': 8, 'relations': []}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    analyzer = RadGraphAnalyzer()\n",
    "    features = analyzer.extract_features(sample_extract)\n",
    "    \n",
    "    print(\"\\nExtracted Features:\")\n",
    "    for key, value in features.items():\n",
    "        print(f\"\\n{key}:\")\n",
    "        print(value)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
