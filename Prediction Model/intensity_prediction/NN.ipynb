{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import dgl\n",
    "from dgl.nn.pytorch.utils import Identity\n",
    "from dgl.nn import GATConv\n",
    "from dgl.nn import EdgeGATConv\n",
    "\n",
    "class ResidualEdgeGATConv(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_feats,\n",
    "            out_feats,\n",
    "            num_heads=\"auto\",\n",
    "            edge_feats=None,\n",
    "            bias=True,\n",
    "            residual=True,\n",
    "            residual_type=\"add\",\n",
    "            force_residual_trafo=True,\n",
    "            **kwargs):\n",
    "        \"\"\"Wrapper class for the GAT graph convolution.\n",
    "            If multiple heads are used, the results are concatenated to one final embedding.\n",
    "\n",
    "        Args:\n",
    "            in_feats (Union[int, (int, int)]): Input feature size.\n",
    "            out_feats (int): Output feature size.\n",
    "            num_heads (Union[str, int], optional): Number of attention heads. Defaults to \"auto\",\n",
    "                which means that there will be one head per 32 features.\n",
    "            edge_feats (int, optional): Number of edge features. Defaults to None,\n",
    "                which means that there are no edge features.\n",
    "            bias (bool, optional): Use bias. Defaults to True.\n",
    "            residual (bool, optional): Use residual. Defaults to True.\n",
    "            residual_type (str, optional): Residual type, \"add\" and \"concat\" are possible. Defaults to \"add\".\n",
    "            force_residual_trafo (bool, optional): Force transformation for residual connection\n",
    "                independent of matching input and output sizes. Defaults to True.\n",
    "        \"\"\"\n",
    "        super(ResidualEdgeGATConv, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.out_feats = out_feats\n",
    "        self.num_heads = num_heads\n",
    "        self.edge_feats = edge_feats\n",
    "        self.residual = residual\n",
    "        self.residual_type = residual_type\n",
    "        self.force_residual_trafo = force_residual_trafo\n",
    "\n",
    "        # Automatically determine number of attention heads\n",
    "        if num_heads == \"auto\":\n",
    "            if isinstance(in_feats, tuple):\n",
    "                in_feat_size = in_feats[1]\n",
    "            else:\n",
    "                in_feat_size = in_feats\n",
    "            num_heads = int(math.ceil(in_feat_size / 32))\n",
    "            self.num_heads = num_heads\n",
    "\n",
    "        # Use default GATConv if there are no edge features and EdgeGATConv if there are edge features\n",
    "        # Residual and bias are set to false, because they are handled in this wrapper class separately\n",
    "        if self.edge_feats is None:\n",
    "            self.conv = GATConv(in_feats, int(out_feats / num_heads), num_heads,\n",
    "                                allow_zero_in_degree=True, residual=False, bias=False)\n",
    "        else:\n",
    "            self.conv = EdgeGATConv(in_feats, int(out_feats / num_heads), num_heads, edge_feats,\n",
    "                                    allow_zero_in_degree=True, residual=False, bias=False)\n",
    "\n",
    "        if residual and residual_type == \"concat\":\n",
    "            # Take destination feature size as the input size for the residual connection\n",
    "            if isinstance(in_feats, tuple):\n",
    "                in_feats = in_feats[1]\n",
    "\n",
    "            if force_residual_trafo:\n",
    "                self.res = torch.nn.Linear(\n",
    "                    in_feats + out_feats, out_feats, bias=False)\n",
    "            else:\n",
    "                if in_feats != out_feats:\n",
    "                    self.res = torch.nn.Linear(\n",
    "                        in_feats + out_feats, out_feats, bias=False)\n",
    "                else:\n",
    "                    self.res = Identity()\n",
    "\n",
    "        if residual and residual_type == \"add\":\n",
    "            # Take destination feature size as the input size for the residual connection\n",
    "            if isinstance(in_feats, tuple):\n",
    "                in_feats = in_feats[1]\n",
    "\n",
    "            if force_residual_trafo:\n",
    "                self.res = torch.nn.Linear(in_feats, out_feats, bias=False)\n",
    "            else:\n",
    "                if in_feats != out_feats:\n",
    "                    self.res = torch.nn.Linear(in_feats, out_feats, bias=False)\n",
    "                else:\n",
    "                    self.res = Identity()\n",
    "\n",
    "        if bias:\n",
    "            self.bias = torch.nn.parameter.Parameter(torch.zeros(out_feats))\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "\n",
    "    def forward(self, graph, feat, edge_feat=None, get_attention=False):\n",
    "        \"\"\"Forward pass of the GAT graph convolution.\n",
    "\n",
    "        Args:\n",
    "            graph (dgl.heterograph.DGLHeteroGraph): Input graph.\n",
    "            feat ((torch.Tensor, torch.Tensor)): Tuple containing source\n",
    "                and destination node features.\n",
    "            edge_feat (torch.Tensor, optional): Edge features. Defaults to None.\n",
    "            get_attention (bool, optional): Return attention weights. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output features of shape :math:`(N, out_feats)`\n",
    "                where :math:`N` corresponds to the number of destination nodes.\n",
    "            torch.Tensor, optional: Attention weights of shape :math:`(E, 1)`\n",
    "                where :math:`E` corresponds to the number of edges.\n",
    "        \"\"\"        \n",
    "        if get_attention and edge_feat is None:\n",
    "            x, a = self.conv(graph, feat, get_attention=True)\n",
    "        elif not get_attention and edge_feat is None:\n",
    "            x = self.conv(graph, feat, get_attention=False)\n",
    "        elif get_attention and edge_feat is not None:\n",
    "            x, a = self.conv(graph, feat, edge_feat, get_attention=True)\n",
    "        elif not get_attention and edge_feat is not None:\n",
    "            x = self.conv(graph, feat, edge_feat, get_attention=False)\n",
    "\n",
    "        # Concat heads, e.g., [d, 4, 16] -> [d, 64]\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        if self.residual and self.residual_type == \"concat\":\n",
    "            # Take destination features as the input for the residual connection\n",
    "            if isinstance(feat, tuple):\n",
    "                feat = feat[1]\n",
    "\n",
    "            x = self.res(torch.cat((x, feat), dim=-1))\n",
    "\n",
    "        if self.residual and self.residual_type == \"add\":\n",
    "            # Take destination features as the input for the residual connection\n",
    "            if isinstance(feat, tuple):\n",
    "                feat = feat[1]\n",
    "\n",
    "            x = x + self.res(feat)\n",
    "\n",
    "        # Add bias\n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "\n",
    "        if get_attention:\n",
    "            a = torch.flatten(a, start_dim=1)\n",
    "            return x, a\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "import pytorch_lightning as pl \n",
    "\n",
    "class SCENE_model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size,\n",
    "        num_heads,\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.0,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Model parameters\n",
    "        # in_nodes is not used here because we are not initializing embeddings, we are taking them from graph \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "        # Node feature encoders \n",
    "        # These encoders are explicitly projected into the hidden_size required \n",
    "        # linear transformation of the patient original in_feats to hidden_size \n",
    "        self.patient_encoder = torch.nn.Linear(768, hidden_size)  \n",
    "        self.topic_encoder = torch.nn.Linear(5, hidden_size)     \n",
    "        # linear transformation of the topic original in_feats to hidden_size \n",
    "\n",
    "        # Explicitly define the convolutions for every edge type; assign separate ResidualEdgeGATConv \n",
    "        # layers are defined by edge type,NOT in layers which are distinguished by target node type \n",
    "        \n",
    "        # topic convolution has single binary edge\n",
    "        self.topic_conv = ResidualEdgeGATConv(\n",
    "            in_feats=hidden_size,\n",
    "            out_feats=hidden_size,\n",
    "            num_heads=num_heads\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # topic to patients have weighted relationship by distribution \n",
    "        self.topic_to_patient_conv = ResidualEdgeGATConv(\n",
    "            in_feats=hidden_size,\n",
    "            out_feats=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.patient_classification_conv = ResidualEdgeGATConv(\n",
    "            in_feats=hidden_size,\n",
    "            out_feats=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "\n",
    "        self.patient_diagnosis_conv = ResidualEdgeGATConv(\n",
    "            in_feats=hidden_size,\n",
    "            out_feats=hidden_size,\n",
    "            num_heads=num_heads\n",
    "            )\n",
    "            \n",
    "    def forward(self, g):\n",
    "            # Node encoding stays the same\n",
    "            g.nodes['patient'].data['h'] = self.patient_encoder(g.nodes['patient'].data['feat'])\n",
    "            g.nodes['topic'].data['h'] = self.topic_encoder(g.nodes['topic'].data['feat'])\n",
    "\n",
    "            # Binary edges - no weights needed\n",
    "            topic_subgraph = g.edge_type_subgraph([('topic', 'converges_with', 'topic')])\n",
    "            topic_feats = g.nodes['topic'].data['h']\n",
    "            g.nodes['topic'].data['h'] = F.relu(\n",
    "                self.topic_conv(topic_subgraph, (topic_feats, topic_feats))\n",
    "            )\n",
    "\n",
    "            # For weighted edges, incorporate weights into attention\n",
    "            topic_to_patient_subgraph = g.edge_type_subgraph([('topic', 'linked_to', 'patient')])\n",
    "            topic_feats = g.nodes['topic'].data['h']\n",
    "            patient_feats = g.nodes['patient'].data['h']\n",
    "            # Add weights directly to the subgraph instead of passing as edge features\n",
    "            topic_to_patient_subgraph.edata['a'] = topic_to_patient_subgraph.edata['weight']\n",
    "            g.nodes['patient'].data['h'] = F.relu(\n",
    "                self.topic_to_patient_conv(topic_to_patient_subgraph, (topic_feats, patient_feats))\n",
    "            )\n",
    "\n",
    "            # Same pattern for other weighted edges\n",
    "            patient_classification_subgraph = g.edge_type_subgraph([('patient', 'classification_similarity', 'patient')])\n",
    "            patient_feats = g.nodes['patient'].data['h']\n",
    "            patient_classification_subgraph.edata['a'] = patient_classification_subgraph.edata['weight']\n",
    "            g.nodes['patient'].data['h'] = F.relu(\n",
    "                self.patient_classification_conv(patient_classification_subgraph, (patient_feats, patient_feats))\n",
    "            )\n",
    "            \n",
    "            # Binary citation edges\n",
    "            patient_diagnosis_subgraph = g.edge_type_subgraph([('patient', 'diagnosiss', 'patient')])\n",
    "            patient_feats = g.nodes['patient'].data['h']\n",
    "            g.nodes['patient'].data['h'] = F.relu(\n",
    "                self.patient_diagnosis_conv(patient_diagnosis_subgraph, (patient_feats, patient_feats))\n",
    "            )\n",
    "\n",
    "            return g.nodes['patient'].data['h']"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
